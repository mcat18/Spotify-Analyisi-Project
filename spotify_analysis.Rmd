---
title: Spotify Analysis
#author: "Mary Aldrete"
output:
  html_document:
    toc: TRUE
    code_folding: hide
---


<style>
.nav>li>a {
    position: relative;
    display: block;
    padding: 10px 15px;
    color: #000000;
}
.nav-pills>li.active>a, .nav-pills>li.active>a:hover, .nav-pills>li.active>a:focus {
    color: #FFFFFF;
    background-color: #1DB954;
}


</style>

<style>
a:link {
    color: black;
}

a:visited {
    color: black;
}

 a:hover {
    color: 	#1DB954;
}

</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Loading Packages


Packages used in analysis


```{r, results='hide',warning=FALSE,message=FALSE}
#Loading packages
# Package names
packages <- c("tidyverse", "ggcorrplot", "randomForest", "here", "knitr", "tree", "gbm", 
              "e1071", "randomForestSRC", "caret", "ROCR", "car", 
              "rpart", "readr", "DescTools", "MASS", "rstatix", "viridis", 
              "rattle", "pdp","rmarkdown")
# Install packages not yet installed
installed_packages <- packages %in% rownames(installed.packages())
if (any(installed_packages == FALSE)) {
  install.packages(packages[!installed_packages])
}
# Packages loading
invisible(lapply(packages, library, character.only = TRUE))
```




# Reading the data

```{r}
spotify = read.csv("data.csv", header = TRUE)
```



# Data Cleaning {.tabset .tabset-fade .tabset-pills}

## Data Types

Checking the structure of the data to make sure the data types of the variables are correct. 

```{r}
str(spotify)
```

Changing the variable type and changing the variables values. 

```{r}
spotify$explicit[spotify$explicit == 1] = "Explicit"
spotify$explicit[spotify$explicit == 0] = "Not explicit"
spotify$explicit = as.factor(spotify$explicit)

spotify$mode[spotify$mode == 1] = "Major"
spotify$mode[spotify$mode == 0] = "Minor"
spotify$mode = as.factor(spotify$mode)



spotify$key = as.factor(spotify$key)

```

Rechecking the structure to make sure the changes were correctly implemented. 

```{r}
str(spotify)
```

## Missing Values

There are no missing values

```{r}
sum(is.na(spotify))
```
## Filtering Data

Filtering the data to use songs only from the past 5 years

```{r}
spotify.past.5 = spotify %>% 
  filter(year == 2016 | year == 2017 |
           year == 2018 | year == 2019 | year == 2020)
```

# {.unlisted .unnumbered}

# Data Exploration {.tabset .tabset-fade .tabset-pills}

## Count of Song Keys 

Most songs are written in the key of G, C, and C#. The key of D# is used the least. 


```{r}
#changing the factor levels of Key so they show up correctly on the visualization
spotify.past.5$key = factor(spotify.past.5$key, levels = c("7", "0", "1", "9", 
                                                           "2", "11", "5", "4",
                                                           "6", "10", "8", "3"))
ggplot(spotify.past.5, aes(x = forcats::fct_infreq(key)))   +
  geom_bar(aes(fill = key), stat = "count", position = "dodge") +
  labs(title= "Count of Key of Songs from 2016-2020", y = "Count", x = "Key") +
  scale_color_viridis(discrete = TRUE) +
  scale_y_continuous(label = scales::comma) +
  scale_x_discrete(breaks = c(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10,11), labels = c("C", "C#", "D", "D#", "E", "F", "F#", "G", "G#", "A", "A#", "B")) +
  scale_fill_manual(values = c("#440154", "#482173", "#433e85", "#38588c", "#2d708e",
                               "#25858e", "#1e9b8a", "#2ab07f", "#52c569", "#86d549", 
                               "#c2df23", "#fde725")) +
  theme(legend.position="none") 

```

## Count of Mode

Most songs are Major. 


```{r}


ggplot(spotify.past.5, aes(x = mode)) +
  geom_bar(aes(fill = mode), stat = "count", position = "dodge") +
  labs(title= "Count of Mode of Songs from 2016-2020", y = "Count", x = "Mode") +
  scale_y_continuous(label = scales::comma) +
  scale_x_discrete(breaks = c("Major", "Minor"), labels = c("Major", "Minor")) +
  scale_fill_manual(values = c("#440154", "#fde725")) +
  theme(legend.position="none") 

```

## Count of Explicit and Non Explicit

Most songs are not explicit


```{r}
#changing the factor levels of Explicit so they show up correctly on the visualization
spotify.past.5$explicit = factor(spotify.past.5$explicit, levels = c("Not explicit","Explicit"))

ggplot(spotify.past.5, aes(x = explicit)) +
  geom_bar(aes(fill = explicit), stat = "count", position = "dodge") +
  labs(title= "Count of Explicit and Non Explicit Songs from 2016-2020", y = "Count", x = "Explicit") +
  scale_y_continuous(label = scales::comma) +
  scale_x_discrete(breaks = c("Explicit", "Not explicit"), labels = c("Explicit", "Not Explicit")) +
  scale_fill_manual(values = c("#440154", "#fde725")) +
  theme(legend.position="none") 


```


## Correlations



Examining the correlations of the numeric variables.

Acousticness and energy have a high negative correlation (r = -0.68) meaning as the acousticness increases, energy decreases, and acousticness and loudness have a high negative correlation (r = -52) meaning as the acoustincess increases, loudness decreases.

```{r, results='hide'}
cordata3 <- spotify.past.5 %>% 
  dplyr::select(acousticness, danceability, energy, duration_ms, instrumentalness, valence, tempo, liveness, loudness, speechiness)

cormatrix3 <- cor(cordata3)

round(cormatrix3, 2)
```

```{r}
ggcorrplot(cormatrix3, hc.order = TRUE,outline.color = "white", lab = TRUE, colors = c("#52c569", "white", "#fde725"), lab_size = 2.5) +
  labs(title="Correlation of Numeric Variables") 
```


## Histograms of Numeric Variables

The distribution of numeric variables. In the histogram, we can observe that acoustincess, duration, instrumentalness, liveness, popularity, speechiness are left skewed. Energy, loudness, and danceability are right skewed. 


```{r, warning=FALSE, message=FALSE}
spotify.past.5%>%
  keep(is.numeric) %>% 
  gather() %>% 
  ggplot(aes(value)) +
  facet_wrap(~ key, scales = "free") +
  geom_histogram(aes(y=..density..), colour="black", fill="white")+
  geom_density(alpha=.2, fill="#3399CC") +
   labs(title= "Distribution of Numeric Variables")

```


## Distribution of Mode

The distribution of popularity in the major or minor mode. The have the exact distributions.


```{r}
ggplot(spotify.past.5, aes(y = popularity, x = mode, fill = mode)) + 
  geom_violin() +
  labs(title= "Distribution of Mode of Songs from 2016-2020", y = "Popularity", x = "Mode") +
  scale_fill_manual(values = c("#440154", "#fde725")) +
  theme(legend.position="none")

```

## Distribution of Explicit


The distribution of explicit and non explicit songs. Most explicit songs fall into the high popularity range and non explixit songs fall in the low populairty range. 

```{r}
ggplot(spotify.past.5, aes(y = popularity, x = explicit, fill = explicit)) + 
  geom_violin() +
  labs(title= "Distribution of Explicit of Songs from 2016-2020", y = "Popularity", x = "Explicit") +
  scale_fill_manual(values = c("#440154", "#fde725")) +
  theme(legend.position="none")
```


## Distribution of Keys


The popularity distribution of the key. The keys have a very similar distribution in popularity. 

```{r}
ggplot(spotify.past.5, aes(y = popularity, x = key, fill = key)) + 
  geom_violin() +
  labs(title= "Distribution of Key of Songs from 2016-2020", y = "Popularity", x = "Key") +
 # scale_fill_discrete(name = "Key", labels = c("C", "C#", "D", "D#", "E", "F", "F#", "G", "G#", "A", "A#", "B")) +
  scale_x_discrete(breaks = c(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10,11), labels = c("C", "C#", "D", "D#", "E", "F", "F#", "G", "G#", "A", "A#", "B")) +
  scale_fill_manual(values = c("#440154", "#482173", "#433e85", "#38588c", "#2d708e",
                               "#25858e", "#1e9b8a", "#2ab07f", "#52c569", "#86d549", 
                               "#c2df23", "#fde725")) +
  theme(legend.position="none")
```

# {.unlisted .unnumbered}

# Splitting Data for Test and Train

Split data into 80% train and 20% test 

```{r}
set.seed(1)

idx.train.past.5 = createDataPartition(y=spotify.past.5$popularity, p =.8, list =F)
train.past.5 = spotify.past.5[idx.train.past.5,]
test.past.5 = spotify.past.5[-idx.train.past.5,]
```



# Linear Regression {.tabset .tabset-fade .tabset-pills}

## Linear Regression Results

```{r, results='hide'}

set.seed(1)

lm.model.past.5 = lm(popularity ~ acousticness + danceability + duration_ms + energy + explicit +
                instrumentalness + key + liveness + loudness + mode + speechiness + 
                tempo + valence, data = train.past.5)

summary(lm.model.past.5)
```


```{r}
lm.model.past.5$fitted.values
```



```{r,echo=FALSE}
regression_results = 
  data.frame(Variable = c("Acousticness",
                          "Danceability", 
                          "Duration", 
                          "Energy", 
                          "Explicit", 
                          "Instrumentalness", 
                          "Key of C#", 
                          "Key of D", 
                          "Key of D#", 
                          "Key of E", 
                          "Key of F",
                          "Key of F#", 
                          "Key of G", 
                          "Key of G#", 
                          "Key of A", 
                          "Key of A#", 
                          "Key of B", 
                          "Liveness", 
                          "Loudness", 
                          "Mode:Minor", 
                          "Speechiness", 
                          "Tempo", 
                          "Valence"), 
             Coefficients = c(4.650e+00, 
                              9.344e+00, 
                              -1.981e-05, 
                              -3.087e+01, 
                              2.548e+01,
                              -1.698e+01,
                              3.157e+00,
                              -1.119e+00,
                              5.052e-01,
                              -9.454e-01,
                              2.722e+00,
                              3.451e+00,
                              -3.972e-01,
                              3.090e+00,
                              -1.492e+00,
                              9.686e-01,
                              1.031e+00,
                              -1.081e+01,
                              1.680e+00,
                              -4.885e-03,
                              -1.626e+01,
                              -4.138e-02, 
                              -6.664e+00), 
             Standard_Error = c(1.178e+00,
                                1.756e+00,
                                1.460e-06,
                                1.942e+00,
                                7.049e-01,
                                8.464e-01,
                                1.048e+00,
                                1.101e+00,
                                1.703e+00,
                                1.179e+00,
                                1.148e+00,
                                1.162e+00,
                                1.031e+00,
                                1.195e+00,
                                1.082e+00,
                                1.190e+00,
                                1.141e+00,
                                1.299e+00,
                                9.598e-02,
                                5.370e-01,
                                2.389e+00,
                                9.421e-03,
                                1.127e+00), 
             t_value = c(3.948,
                         5.321,
                         -13.574,
                         -15.893,
                         36.154,
                         -20.061,
                         3.012,
                         -1.017,
                         0.297,
                         -0.802,
                         2.371,
                         2.970,
                         -0.385,
                         2.587,
                         -1.379,
                         0.814,
                         0.903,
                         -8.317,
                         17.508,
                         -0.009,
                         -6.808,
                         -4.393,
                         -5.910), 
             p_value = c("7.93e-05 ***", 
                         "1.05e-07 ***", 
                         "< 2e-16 ***", 
                         "< 2e-16 ***",
                         "< 2e-16 ***",
                         "< 2e-16 ***",
                         "0.00260 **",
                         "0.30924",
                         "0.76667",
                         "0.42260", 
                         "0.01775 *", 
                         "0.00299 **",
                         "0.70018",
                         "0.00969 **",
                         "0.16793",
                         "0.41554",
                         "0.36639", 
                         "< 2e-16 ***",
                         "< 2e-16 ***",
                         "0.99274", 
                         "1.04e-11 ***", 
                         "1.13e-05 ***", 
                         "3.51e-09 ***"))
```



Acousticness, danceability, duration, energy, explicit, instrumentalness, key of C#, key of F, key of F#, key of G, liveness, loudness, speechiness, tempo, and valence are statisically significant. 


```{r}
kable(regression_results, 
      col.names = c("Variable","Coefficients", "Standard Error", "t-value", "p-value"))
```


The model explains 32.81% of the variation in the data. This is a very low adjusted r-squared value which indicates this is not a good model. The model is statistically significant with a p-value below .05. 

```{r}
regression_model_info = 
  data.frame(R_squared = 0.3295,
             Adjusted_R_squared = 0.3281, 
             F_statistic = "236.1 on 23 and 11051 DF", 
             F_statistic_p_value = "< 2.2e-16")

kable(regression_model_info, 
      col.names = c("R-squared","Adjusted R-squared", "F-statistic", "F-statistic p-value"))
```


The GVIF is low across variables so there is no multicollinearity. 

```{r}
kable(car::vif(lm.model.past.5))
```



## Residuals 

```{r}
par(mfrow = c(2,2))
plot(lm.model.past.5, which = 1:4)
```


## Prediction and Model Evaluation

Making predictions and evaluating the performance of the model. 

```{r}
predictions3 = predict(lm.model.past.5, newdata = test.past.5, type = "response")

mse3 = mean((predictions3 - test.past.5$popularity)^2)

lm.rmse = caret::RMSE(predictions3,test.past.5$popularity)

lm.r2 = R2(predictions3, test.past.5$popularity)

regression_performance_metrics = 
  data.frame(R_squared = lm.r2,
             MSE = mse3,
             RMSE = lm.rmse)

kable(regression_performance_metrics, 
      col.names = c("R-squared","MSE","RMSE"))
```


This model has a low r-squared, so the model is not explaining much of the variation. 


# {.unlisted .unnumbered}



# Support Vector Regression {.tabset .tabset-fade .tabset-pills}


## Support Vector Regression Results

```{r}
set.seed(1)

svm.model = svm(popularity ~ acousticness + danceability + duration_ms + energy + explicit +
                instrumentalness + key + liveness + loudness + mode + speechiness + 
                tempo + valence, data = train.past.5)

svm.model
```

## Prediction and Model Evaluation

Making predictions on the test data and evaluating the model. 

The SVR model accounts for 47.43% of the variation in the data and has a lower MSE and RMSE than the linear regression. This is an improvement from the linear regression. 


```{r}

predSVM = predict(svm.model, test.past.5, type = "response")

#Compute error 
svm.mse = mean((predSVM - test.past.5$popularity)^2)

svm.rmse = caret::RMSE(predSVM,test.past.5$popularity)

svm.r2 = R2(predSVM, test.past.5$popularity)

svm_performance_metrics = 
  data.frame(R_squared = svm.r2,
             MSE = svm.mse,
             RMSE = svm.rmse)


kable(svm_performance_metrics, 
      col.names = c("R-squared", "MSE", "RMSE"))
```


# {.unlisted .unnumbered}


# Decision Tree {.tabset .tabset-fade .tabset-pills}

## Decision Tree Results

```{r}
set.seed(1)
dt.model <- rpart(popularity ~ acousticness + danceability + duration_ms + energy +
                instrumentalness + liveness + loudness + speechiness + 
                tempo + valence + key + explicit + mode, data = train.past.5, method = "anova") # simple DT model

rattle::fancyRpartPlot(dt.model, sub = "")
```

## Prediction and Model Evaluation

Making predictions on the test data and evaluating the model.

The decision tree accounts for 32.85% of the variation in the data. The model has a similar MSE and RMSE as the linear regression. 

```{r}
preds_DT = predict(dt.model, test.past.5, type = "vector")
mse.dt = mean((preds_DT- test.past.5$popularity)^2)

dt.rmse =caret::RMSE(preds_DT,test.past.5$popularity)

dt.r2 = R2(preds_DT, test.past.5$popularity)

dt_performance_metrics = 
  data.frame(R_squared = dt.r2, 
             MSE = mse.dt,
             RMSE = dt.rmse)

kable(dt_performance_metrics, 
      col.names = c("R-squared", "MSE", "RMSE"))
```


## Pruning the Tree

cp value of 0.01 has the lowest error. This will be used to prune the original tree. 

```{r}
printcp(dt.model)

best.cp = dt.model$cptable[which.min(dt.model$cptable[,"xerror"]),"CP"]

plotcp(dt.model)
```




## Pruned Decision Tree Results 

```{r}
set.seed(1)
pruned.tree = prune(dt.model, cp = best.cp)
rattle::fancyRpartPlot(pruned.tree, sub = "")
```


## Prediction and Model Evaluation

Making predictions on the test data and evaluating the model.


Pruning the tree made no difference in the performance of the model.

```{r}
preds_DT_pruned = predict(pruned.tree, test.past.5, type = "vector")

mse.dt.pruned = mean((preds_DT_pruned- test.past.5$popularity)^2)

dt.pruned.rmse = caret::RMSE(preds_DT_pruned,test.past.5$popularity)

dt.pruned.r2 = R2(preds_DT_pruned, test.past.5$popularity)

pruned_performance_metrics = 
  data.frame(R_squared = dt.pruned.r2, 
             MSE = mse.dt.pruned,
             RMSE = dt.pruned.rmse)


kable(pruned_performance_metrics, 
      col.names = c("R-squared", "MSE", "RMSE"))
```


# {.unlisted .unnumbered}


# Random Forest {.tabset .tabset-fade .tabset-pills}

## Random Forest Results


```{r}
set.seed(1)
rf.model = randomForest(popularity ~ acousticness + danceability + duration_ms + energy + instrumentalness + liveness + loudness + speechiness + tempo + valence + key + explicit + mode, data = train.past.5, importance = TRUE)

print(rf.model)
```

## Variable Importance

```{r}
varImpPlot(rf.model)
```

```{r, echo=FALSE, results='hide'}
rf.model$importance[,1]
```

```{r, echo=FALSE, results='hide'}
rf.model$importance
```

## Predictions and Model Evaulation 

Making predictions on the test data and evaluating the model.


The random forest accounts for 57.54% of the variation in the data, and has the lowest MSE and RMSE compared to the previous models.


```{r}
y_hat = predict(rf.model, test.past.5)


mse.rf = mean((y_hat - test.past.5$popularity)^2)


rf.rmse = caret::RMSE(y_hat,test.past.5$popularity)

rf.r2 = R2(y_hat, test.past.5$popularity)

rf_performance_metrics = 
  data.frame(R_squared = rf.r2, 
             MSE = mse.rf,
             RMSE = rf.rmse)


kable(rf_performance_metrics, 
      col.names = c("R-squared", "MSE", "RMSE"))
```


## Explicit PDP Plot


Explicit songs are predicted to be more popular. 


```{r}
partialPlot(rf.model, train.past.5, explicit, popularity)
```


## Instrumentalness PDP Plot

The more a song is instrumental, the less likely it will be predicted high on popularity.  


```{r}
partialPlot(rf.model, train.past.5, instrumentalness, popularity)
```


## Duration PDP Plot

Shorter songs are predicted to be more popular. 

```{r}
partialPlot(rf.model, train.past.5, duration_ms, popularity)
```

## Loudness PDP Plot

Louder songs are predicted to be more popular

```{r}
partialPlot(rf.model, train.past.5, loudness, popularity)
```



## Energy PDP Plot

Having a low and moderate energy predicts higher popularity than songs that are highly energetic. 

```{r}

partialPlot(rf.model, train.past.5, energy, popularity)
```

## Acousticness PDP Plot

Moderate to high acousticness predicts higher popularity. 

```{r}
partialPlot(rf.model, spotify.past.5, acousticness, popularity)
```

# {.unlisted .unnumbered}


# Model Comparisons {.tabset .tabset-fade .tabset-pills}


## MSE

```{r, echo=FALSE}
models = data.frame(Method = c("Linear Regression", "Support Vector Regression", "Decision Tree", "Pruned Decision Tree", "Random Forest"), 
           MSE = c(round(mse3, 2), round(svm.mse, 2), round(mse.dt, 2), round(mse.dt.pruned,2), round(mse.rf,2)), 
           RMSE = c(round(lm.rmse,2), round(svm.rmse,2), round(dt.rmse,2), round(dt.pruned.rmse,2), round(rf.rmse,2)), 
           RSquared = c(round(lm.r2,2), round(svm.r2,2), round(dt.r2,2), round(dt.pruned.r2,2), round(rf.r2,2)))
```


Linear regression has the highest MSE and random forest has the lowest MSE. 

```{r}
ggplot(models, aes(x = reorder(Method, -MSE), y = MSE, fill = Method)) + 
  geom_bar(stat = "identity") +
  theme(axis.text.x = element_text(angle = 20, hjust = 0.5)) +
   scale_y_continuous(limits = c(0,700)) +
  labs(title= "MSE of Models", y = "MSE", x = "Method") +
  geom_text(aes(label=MSE), position=position_dodge(width=0.9), vjust=-0.25) +
  scale_fill_manual(values = c("#3b528b", "#440154", "#21918c", 
                               "#fde725", "#5ec962")) +
  theme(legend.position="none")
 
  
```


## RMSE

Linear Regression has the highest RMSE and random forest has the loweest RMSE. 

```{r}
ggplot(models, aes(x = reorder(Method, -RMSE), y = RMSE, fill = Method)) + 
  geom_bar(stat = "identity") +
  theme(axis.text.x = element_text(angle = 20, hjust = 0.5)) +
  labs(title= "RMSE of Models", y = "RMSE", x = "Method") +
   scale_y_continuous(limits = c(0,30)) +
  geom_text(aes(label=RMSE), position=position_dodge(width=0.9), vjust=-0.25) +
    scale_fill_manual(values = c("#3b528b", "#440154", "#21918c", 
                               "#fde725", "#5ec962")) +
  theme(legend.position="none")
  
```

## R-squared


 Random forest has the highest R-squared and the decision tree, linear regression, and pruned decision tree have the lowest RMSE. 

```{r}
ggplot(models, aes(x = reorder(Method, -RSquared), y = RSquared, fill = Method)) + 
  geom_bar(stat = "identity") +
  theme(axis.text.x = element_text(angle = 20, hjust = 0.5)) +
  labs(title= "R-Sqaured of Models", y = "R-Sqaured", x = "Method") +
  scale_y_continuous(limits = c(0, .7)) +
  geom_text(aes(label=RSquared), position=position_dodge(width=0.9), vjust=-0.25) +
    scale_fill_manual(values = c("#21918c", "#5ec962", "#fde725", 
                               "#440154", "#3b528b")) +
  theme(legend.position="none")
  
```



# {.unlisted .unnumbered}

























